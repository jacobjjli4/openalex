{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VazxsCzcknt"
      },
      "source": [
        "# Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xb-kiBDqo77J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"YOUR PATH HERE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9nELZ1nlpNE5"
      },
      "outputs": [],
      "source": [
        "filename = \"openalex_names_full_study_df10_0_100000_slim.csv\"\n",
        "path = \"\"\n",
        "in_df = pd.read_csv(path+filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BFhDJ28_oI5E"
      },
      "outputs": [],
      "source": [
        "# add headers so that code is added into OpenAlex's \"polite\" pool\n",
        "headers = {'mailto':'YOUR EMAIL HERE'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkieELdEpGD3"
      },
      "source": [
        "# Defining Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lR-ArtXWq9u4"
      },
      "outputs": [],
      "source": [
        "class Bucket:\n",
        "    '''\n",
        "    A token bucket to rate limit API requests.\n",
        "    '''\n",
        "    def __init__(self, max_tokens, refill_rate):\n",
        "        \"\"\"\n",
        "        Initialize a new token bucket with a maximum number of tokens\n",
        "        max_tokens which regenerate after refill_rate seconds. tokens represents\n",
        "        the number of tokens currently in the bucket. current_time represents\n",
        "        the current time and is used in refill behaviour.\n",
        "\n",
        "        token_checkout_times is a list of the last ten times that a token was\n",
        "        checked out and is initialized to current_time repeated 10 times. In the\n",
        "        course of operating API calls, token_checkout_times will be ordered in\n",
        "        an increasing order. That is, the earliest times (smallest values)\n",
        "        appear first. Old checkout times are removed from the beginning of the\n",
        "        list (index 0) and new checkout times are added to the end of the list\n",
        "        (index 9).  As such, token_checkout_times should almost always have\n",
        "        length 10, but it may be less than 10 if API calls take a long time\n",
        "        to come back (i.e., new API calls are made before some API calls\n",
        "        are returned). token_checkout_times will be constantly updated so that\n",
        "        refill can check if enough time has elapsed before refilling the token\n",
        "        bucket.\n",
        "\n",
        "        An API request can only be made if tokens is greater than 1.\n",
        "        \"\"\"\n",
        "\n",
        "        self.max_tokens = max_tokens\n",
        "        self.tokens = max_tokens\n",
        "        self.current_time = time.perf_counter()\n",
        "        self.token_checkout_times = [self.current_time] * self.max_tokens\n",
        "        self.refill_rate = refill_rate\n",
        "\n",
        "    def refill(self):\n",
        "        \"\"\"\n",
        "        Increment the token count by 1 if the current time is at least\n",
        "        refill_rate after the 10th last token was checked out (the first item\n",
        "        in token_checkout_times).\n",
        "\n",
        "        >>> bucket = Bucket(10, 2)\n",
        "        >>> bucket.get_token()\n",
        "        >>> bucket.tokens == 9\n",
        "        True\n",
        "        >>> bucket.refill()\n",
        "        >>> bucket.tokens == 9\n",
        "        True\n",
        "        >>> time.sleep(bucket.refill_rate)\n",
        "        >>> bucket.refill()\n",
        "        >>> bucket.tokens == 10\n",
        "        True\n",
        "        \"\"\"\n",
        "        self.current_time = time.perf_counter()\n",
        "        while len(self.token_checkout_times) == 0:\n",
        "            time.sleep(0.01)\n",
        "            self.current_time = time.perf_counter()\n",
        "        if self.current_time - self.token_checkout_times[0] >= self.refill_rate:\n",
        "            self.tokens += 1\n",
        "\n",
        "    def get_token(self):\n",
        "        \"\"\"\n",
        "        If tokens < 1, then continue calling refill to attempt refilling the\n",
        "        token bucket. This prevents the API call from being made.\n",
        "\n",
        "        If tokens >= 1, then \"check out\" a token by decreasing tokens by 1 and\n",
        "        removing the first (earliest) item of token_checkout_times. After the\n",
        "        API call is succesfully made, functions should call update_checkout_time\n",
        "        to add the time of the successful call to token_checkout_times.\n",
        "\n",
        "        Together with update_checkout_times, get_token can be thought of as\n",
        "        \"pushing out\" the first item of token_checkout_times from the start of\n",
        "        the list and adding the current time to the end of the list.\n",
        "\n",
        "        \"\"\"\n",
        "        while self.tokens < 1:\n",
        "            self.refill()\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        self.token_checkout_times.pop(0)\n",
        "        self.tokens -= 1\n",
        "\n",
        "    def update_checkout_time(self):\n",
        "        \"\"\"\n",
        "        Add the current time to the end of token_checkout_times.\n",
        "        update_checkout_times should only be called after get_token to ensure\n",
        "        that token_checkout_times has 10 elements in the long run.\n",
        "        \"\"\"\n",
        "        self.token_checkout_times = self.token_checkout_times + [time.perf_counter()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fbJbg37DoKHq"
      },
      "outputs": [],
      "source": [
        "WORKS_COLUMNS = ['pub_total', 'pub_id', 'pub_doi', 'pub_title',\n",
        "                  'put_year', 'pub_type', 'pub_cited_by_url,', 'pub_cites',\n",
        "                  'pub_journal_id', 'pub_journal_name',\n",
        "                  'pub_keywords', 'authorship_position', 'pub_facility_id',\n",
        "                  'pub_facility_display_name', 'pub_facility_ror',\n",
        "                  'pub_facility_country','pub_facility_type', 'timestamp_download']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wsz92JBKnXff"
      },
      "outputs": [],
      "source": [
        "def search_works(in_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Return a dataframe (out_df) containing bibliographic information from\n",
        "    OpenAlex about works produced by the authors listed in in_df. in_df must\n",
        "    contain at least the following two columns:\n",
        "     - diss_uid: a unique string identifier for each author assigned by the\n",
        "       author name data source (ex. 1995c3b3df3cd2)\n",
        "     - works_api_url: a URL string containing the link to the OpenAlex directory\n",
        "       containing the author's works\n",
        "       (ex. https://api.openalex.org/works?filter=author.id:A2001663675\n",
        "       corresponds to the author name data source identifier 1995c3b3df3cd2)\n",
        "\n",
        "    out_df is a dataframe with columns WORKS_COLUMNS, each corresponding to some\n",
        "    piece of bibliographic information in the OpenAlex system. Each row will\n",
        "    usually correspond to one publication. More speficifically, if an author\n",
        "    in in_df has:\n",
        "     - NO publications listed on OpenAlex, there will be one row in out_df with\n",
        "       the author's diss_uid and works_api_url. This row will have a 0 entry in\n",
        "       the pub_total column and null entries in all other columns.\n",
        "     - ONE publication listed, there will be one row in out_df with the author's\n",
        "       diss_uid and works_api_url. This row will have a 1 entry in the pub_total\n",
        "       column and entries reflecting its bibliographic information in all other\n",
        "       columns.\n",
        "     - N publications listed (with N > 1), there will be N rows in out_df each\n",
        "       containing the same diss_uid and works_api_url. Each row will correspond\n",
        "       to a different publication, and hence the entries in other columns (ex.\n",
        "       pub_doi or pub_year) will differ between rows.\n",
        "\n",
        "    \"\"\"\n",
        "    # create output dataframe\n",
        "    out_df = pd.DataFrame(columns=['works_api_url', 'diss_uid'] + WORKS_COLUMNS)\n",
        "\n",
        "    # create a token bucket with 10 max tokens and 1 second between refills\n",
        "    bucket = Bucket(10, 1)\n",
        "\n",
        "    # extract URLs to send to OpenAlex API from in_df\n",
        "    df_url_clean = clean_url_list(in_df)\n",
        "    works_url_list = list(df_url_clean['works_api_url'])\n",
        "    diss_uids = list(df_url_clean['diss_uid'])\n",
        "\n",
        "    # execute requests from in_df in parallel\n",
        "    with ThreadPoolExecutor(max_workers=12) as pool:\n",
        "        futures = [pool.submit(query_API_works, url, diss_uid, 200, bucket)\n",
        "                   for (url, diss_uid) in zip(works_url_list, diss_uids)]\n",
        "\n",
        "        # add the result of each hit to out_df\n",
        "        for future in futures:\n",
        "            out_df = pd.concat((out_df, future.result()))\n",
        "\n",
        "    return out_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wh9EYqhBnabR"
      },
      "outputs": [],
      "source": [
        "def query_API_works(works_url: str, diss_uid: str, per_page: int,\n",
        "                    token_bucket: Bucket):\n",
        "    \"\"\"\n",
        "    Return a dataframe (df) containing selected variables corresponding to the\n",
        "    works returned by an OpenAlex search query for works_url. diss_uid is not\n",
        "    used in the API call but is reassigned to df (which contains the search\n",
        "    results) after the API call to help identify which author the results\n",
        "    correspond to. per_page corresponds to the OpenAlex per_page parameter and\n",
        "    controls how many results are displayed per page. token_bucket is\n",
        "    shared between all parallel processes of query_API_works and is used to\n",
        "    rate limit API requests to adhere to OpenAlex's 10 requests per second limit.\n",
        "    \"\"\"\n",
        "    print(f'Starting works query for {works_url}.')\n",
        "    df = pd.DataFrame(columns=WORKS_COLUMNS) # create output df\n",
        "    (pub_total, response_json) = issue_query(works_url, '*', per_page,\n",
        "                                             token_bucket) # send API request\n",
        "\n",
        "    # if 503 error, change per_page to 25 and resend query\n",
        "    while response_json == '<Response [503]>':\n",
        "        per_page = 25\n",
        "        print('503 error encountered. Results per page has been ' +\n",
        "              'changed to 25 (default value: 200)')\n",
        "        (pub_total, response_json) = issue_query(works_url, '*', per_page,\n",
        "                                                 token_bucket)\n",
        "\n",
        "    # record general information about search results\n",
        "    pub_total = response_json['meta']['count']\n",
        "    results = response_json['results']\n",
        "\n",
        "    # if the author has no works, create an entry with no bibliographic info\n",
        "    if pub_total == 0:\n",
        "        df = df.append({'pub_total': 0}, ignore_index=True)\n",
        "\n",
        "    # if all results are on one page, iterate over each result and extract data\n",
        "    elif pub_total <= per_page:\n",
        "        results = response_json['results']\n",
        "        for result in results:\n",
        "            variables = extract_variables_works(works_url, result)\n",
        "            variables = [pub_total] + variables\n",
        "            df.loc[len(df)] = variables\n",
        "\n",
        "    # if more than one page of results, iterate over pages and then over results\n",
        "    else:\n",
        "        for i in range(0, pub_total, per_page):\n",
        "            results = response_json['results']\n",
        "            # extract the cursor to navigate to the next page\n",
        "            next_cursor = response_json['meta']['next_cursor']\n",
        "            for result in results:\n",
        "                variables = extract_variables_works(works_url, result)\n",
        "                variables = [pub_total] + variables\n",
        "                df.loc[len(df)] = variables\n",
        "            # issue query for the next page of results\n",
        "            response_json = issue_query(works_url, next_cursor, per_page,\n",
        "                                        token_bucket)[1]\n",
        "\n",
        "            # if 503 error, change per_page to 25 and resend query\n",
        "            while response_json == '<Response [503]>':\n",
        "                per_page = 25\n",
        "                print('503 error encountered. Results per page has been ' +\n",
        "                      'changed to 25 (default value: 200)')\n",
        "                (pub_total, response_json) = issue_query(works_url, next_cursor,\n",
        "                                                         per_page, token_bucket)\n",
        "\n",
        "    # add the identification parameters (diss_url, diss_uid) back into dataframe\n",
        "    df = add_uid_url(df, works_url, diss_uid)\n",
        "    print(f'Finished query for {works_url}.')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "d8OmsCvmq9vB"
      },
      "outputs": [],
      "source": [
        "def add_uid_url(df: pd.DataFrame, works_url: str, diss_uid: str):\n",
        "    df['works_api_url'] = works_url\n",
        "    df['diss_uid'] = diss_uid\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sbNZgugrmTgm"
      },
      "outputs": [],
      "source": [
        "def issue_query(works_url: str, cursor: str, per_page: int,\n",
        "                token_bucket: Bucket):\n",
        "    search_url = (f'{works_url}&per_page={per_page}&cursor={cursor}')\n",
        "    # attempt to acquire token before launching API call\n",
        "    token_bucket.get_token()\n",
        "    oa_response = requests.get(search_url, headers=headers)\n",
        "    # update the list of checkout times once API call is successful\n",
        "    token_bucket.update_checkout_time()\n",
        "    response_code = str(oa_response)\n",
        "\n",
        "    flag = False\n",
        "    while response_code != '<Response [200]>':\n",
        "        # save the error code to print later\n",
        "        error_code = response_code\n",
        "        print(response_code + search_url)\n",
        "        if response_code == '<Response [503]>':\n",
        "            # pass control back to query_API_works\n",
        "            return (0, response_code)\n",
        "        else:\n",
        "            token_bucket.get_token()\n",
        "        oa_response = requests.get(search_url, headers=headers)\n",
        "        token_bucket.update_checkout_time()\n",
        "        response_code = str(oa_response)\n",
        "        flag = True\n",
        "\n",
        "    if flag:\n",
        "        print('RESOLVED ' + error_code + search_url)\n",
        "\n",
        "    response_json = oa_response.json()\n",
        "    # get total number of hits\n",
        "    pub_total = response_json['meta']['count']\n",
        "\n",
        "    return (pub_total, response_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "t-WpwZnqY8gY"
      },
      "outputs": [],
      "source": [
        "def clean_url_list(in_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    There are some author searches which didn't lead to a works url. We want to remove those from our search\n",
        "    \"\"\"\n",
        "    # in_df['works_api_url'].replace('', np.nan, inplace=True)\n",
        "    in_df.dropna(subset=['works_api_url'], inplace=True)\n",
        "    out_df = in_df[['works_api_url', 'diss_uid']]\n",
        "\n",
        "\n",
        "    return out_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "fH1GYa30namP"
      },
      "outputs": [],
      "source": [
        "def extract_variables_works(works_url: str, result: list):\n",
        "    \"\"\"\n",
        "    Return a list (variables) containing variables of interest corresponding to\n",
        "    a specific OpenAlex API search result (result).\n",
        "    \"\"\"\n",
        "    variables = []\n",
        "\n",
        "    # pull selected variables from each result and add it to variables\n",
        "    # if no such variable exists, leave empty string\n",
        "\n",
        "    #results\n",
        "    ### pub_id\n",
        "    try:\n",
        "        variables.append(result['id'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "    ### pub_doi\n",
        "    try:\n",
        "        variables.append(result['doi'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "    ### pub_title\n",
        "    try:\n",
        "        variables.append(result['title'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "    ### pub_year\n",
        "    try:\n",
        "        variables.append(result['publication_year'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "    ### pub_type\n",
        "    try:\n",
        "        variables.append(result['type'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "    ### pub_cited_by_url\n",
        "    try:\n",
        "        variables.append(result['cited_by_api_url'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "    ### pub_cites\n",
        "    try:\n",
        "        variables.append(result['cited_by_count'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "\n",
        "\n",
        "    # results / host venue\n",
        "    ## pub_journal_id:\n",
        "    try:\n",
        "        variables.append(result['host_venue']['id'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "    ## pub_journal_name:\n",
        "    try:\n",
        "        variables.append(result['host_venue']['display_name'])\n",
        "    except:\n",
        "        variables.append('')\n",
        "\n",
        "    # results / concepts:\n",
        "    ### pub_keywords\n",
        "    concepts = result['concepts']\n",
        "    keywords = []\n",
        "    for concept in concepts:\n",
        "        try:\n",
        "            keywords.append(concept['display_name'])\n",
        "        except:\n",
        "            keywords.append('')\n",
        "\n",
        "    variables.append(keywords)\n",
        "\n",
        "    # results / authorship\n",
        "    ### authorship_position\n",
        "    authorship = result['authorships']\n",
        "\n",
        "    cnt=0\n",
        "    stop_loop = 0\n",
        "    for author in authorship:\n",
        "        author_id_round = works_url.split(\"filter=author.id:\")\n",
        "        author_id_round = author_id_round[1]\n",
        "\n",
        "        try:\n",
        "            author_id_authorship = author['author']['id']\n",
        "        except:\n",
        "            author_id_authorship = \"openalex.org/\"\n",
        "\n",
        "        author_id_authorship = author_id_authorship.split(\"openalex.org/\")\n",
        "        author_id_authorship = author_id_authorship[1]\n",
        "\n",
        "        if (author_id_round == author_id_authorship):\n",
        "            if stop_loop !=1:\n",
        "                try:\n",
        "                    authorship_position = author['author_position']\n",
        "                    variables.append(author['author_position'])\n",
        "                except:\n",
        "                    variables.append('')\n",
        "\n",
        "                ### results / authorship / author\n",
        "\n",
        "                ###results / authorships / institutions\n",
        "                # institutions = authorship['institutions']\n",
        "                # print('institutions',institutions)\n",
        "\n",
        "                try:\n",
        "                    pub_facility_id = author['institutions'][cnt]['id']\n",
        "                    variables.append(author['institutions'][cnt]['id'])\n",
        "                except:\n",
        "                    #print('no institutions info') # --> it says there is no institutions information coded\n",
        "                    variables.append('')\n",
        "\n",
        "                try:\n",
        "                    pub_facility_display_name = author['institutions'][cnt]['display_name']\n",
        "                    variables.append(author['institutions'][cnt]['display_name'])\n",
        "                except:\n",
        "                  variables.append('')\n",
        "\n",
        "                try:\n",
        "                    pub_facility_ror = author['institutions'][cnt]['ror']\n",
        "                    variables.append(author['institutions'][cnt]['ror'])\n",
        "                except:\n",
        "                    variables.append('')\n",
        "\n",
        "                try:\n",
        "                    pub_facility_country = author['institutions'][cnt]['country_code']\n",
        "                    variables.append(author['institutions'][cnt]['country_code'])\n",
        "                except:\n",
        "                    variables.append('')\n",
        "\n",
        "                try:\n",
        "                    pub_facility_type = author['institutions'][cnt]['type']\n",
        "                    variables.append(author['institutions'][cnt]['type'])\n",
        "                except:\n",
        "                    variables.append('')\n",
        "\n",
        "                cnt = cnt+1\n",
        "                stop_loop = 1\n",
        "\n",
        "    if stop_loop ==0:\n",
        "        variables.append('')\n",
        "        variables.append('')\n",
        "        variables.append('')\n",
        "        variables.append('')\n",
        "        variables.append('')\n",
        "        variables.append('')\n",
        "\n",
        "\n",
        "    # Timestamp Download:\n",
        "    import datetime\n",
        "    now = datetime.datetime.now()\n",
        "    variables.append(now.strftime(\"%Y_%m_%d\"))\n",
        "\n",
        "    return variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DccZNiSdx9Yx"
      },
      "source": [
        "# Doctests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5EkHiaGx_bS",
        "outputId": "04ff5a78-5a80-485f-e27b-696e11842952"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=8)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import doctest\n",
        "doctest.testmod()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4kb3IMJ5WJx"
      },
      "source": [
        "# Def Run Functions Multiple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWk19e5A5bIn"
      },
      "outputs": [],
      "source": [
        "def run_functions_multiple(in_df, start, stop, queries_per_file,\n",
        "                           queries_per_call, master_path):\n",
        "    '''\n",
        "    Call run_functions to generate multiple files with query results from in_df.\n",
        "    Each output file will have name beginning with master_path with the index\n",
        "    number of the first query appended to the end (ex. some_path/some_file_120).\n",
        "    master_path should NOT have a file extension (.csv) at the end.\n",
        "\n",
        "    Start querying at start and stop querying at stop. queries_per_file decides\n",
        "    how many queries will have their results written to a specific file, whereas\n",
        "    queries_per_call is passed to run_functions and determines how many queries\n",
        "    are sent to search_names at once. queries_per_file should be greater than\n",
        "    queries_per_call.\n",
        "    '''\n",
        "    if queries_per_file < queries_per_call:\n",
        "        print(f'Please set queries_per_file to be greater than queries_per_call'\n",
        "              f'Function execution terminated.')\n",
        "        return\n",
        "\n",
        "    for i in range(start, stop, queries_per_file):\n",
        "        print(f'Starting on query {str(start)}.')\n",
        "        run_functions(in_df, i, i+queries_per_file, queries_per_call,\n",
        "                      master_path+'openalex_works_full_study_'+str(i)+'_'+str(i+queries_per_file)+'.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCdHtCH1Q7w6"
      },
      "outputs": [],
      "source": [
        "def create_slim_df(in_path: str, out_path: str, columns_to_keep: list):\n",
        "    \"\"\"\n",
        "    Create a dataframe saved to out_path with only the specified columns \n",
        "    (columns_to_keep) from in_path.\n",
        "    \"\"\"\n",
        "    in_df = pd.read_csv(in_path)\n",
        "    in_df = in_df[columns_to_keep]\n",
        "    in_df.to_csv(out_path, index=False, header=True, encoding='utf8')\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EaOpr_b5dX7"
      },
      "outputs": [],
      "source": [
        "def run_functions(in_df: pd.DataFrame, start: int, stop: int, step: int,\n",
        "                  path: str):\n",
        "    '''\n",
        "    Write the results of search_names to path. Take in_df and perform search\n",
        "    queries on the rows from start to stop. Starts a new search using search_names\n",
        "    every step rows to increase performance.\n",
        "\n",
        "    Creates a new file to write output. If file already exists, prompts the user\n",
        "    on whether they want to append new output to existing file. Append is useful\n",
        "    when search_names stops unexpectedly, leaving an incomplete file. The user\n",
        "    would then restart querying at the point where the querying stopped and\n",
        "    would choose to append to an existing file.\n",
        "    '''\n",
        "    # generate one set of headers\n",
        "    df_headers = pd.DataFrame(columns=['diss_name', 'diss_uid']+WORKS_COLUMNS)\n",
        "\n",
        "    # try to create the file if it doesn't already exist\n",
        "    try:\n",
        "        df_headers.to_csv(path, index=False, header=True, mode='x')\n",
        "        print(f'NEW FILE CREATED at {path}. \\n')\n",
        "    except FileExistsError:\n",
        "        print(f'FILE ALREADY EXISTS at {path}. Append instead? \\n')\n",
        "        # request user input to prevent accidental appending\n",
        "        response = input('Y/N: ')\n",
        "        if response.upper() != 'Y':\n",
        "           return\n",
        "\n",
        "    # create slice of in_df\n",
        "    in_df = in_df[start:stop]\n",
        "    # execute search_names function for subslices of in_df determined by step\n",
        "    for i in range(0,len(in_df), step):\n",
        "        print(f'====INITIATING queries {str(i)} to {str(i+step)}====')\n",
        "        df_temp = in_df[i:i+step]\n",
        "        df_results = search_works(df_temp)\n",
        "        print(f'\\nFINISHED queries {str(i)} to {str(i+step)}.', end=' ')\n",
        "        print(f'WRITING to {path}...', end = ' ')\n",
        "        df_results.to_csv(path, index=False, header=False, mode='a',\n",
        "                          encoding='utf8')\n",
        "        print('SUCCESSFULLY wrote to file.')\n",
        "\n",
        "    print(f'\\nSUCCESS in querying items {str(start)} to {str(stop)}. (Query end)')\n",
        "\n",
        "    # create \"slim\" df for next step of OA code\n",
        "    slim_columns = ['pub_facility_id']\n",
        "    slim_path = path[:-4] + '_slim' + '.csv'\n",
        "    create_slim_df(path, slim_path , slim_columns)\n",
        "    print(f'Created slim df at {slim_path} containing columns {str(slim_columns)}.\\n')\n",
        "\n",
        "    return None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KHGHy1gllwad",
        "GjgWj5rCQdPn",
        "KZpHtAXKWcNh",
        "cDdoeLPb5L0N",
        "6B1aZvBy8SIx",
        "41FeGcTb8Zk1"
      ],
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
